{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Components\n",
    "---\n",
    "1. inputs\n",
    "2. weights\n",
    "3. biases\n",
    "4. activation function \n",
    "    1. sigmoid (-1, 1)\n",
    "    2. tanh (0, 1)\n",
    "    3. relu (0, 1)\n",
    "    4. leaky relu \n",
    "\n",
    "Note: \n",
    "1. tanh is always better than sigmoind, except for the output layer when used in binary classificaiton. relu is the best.\n",
    "2. Never use linear activation functions, acts as linear regression. can be used for regression problems in the last layer.\n",
    "\n",
    "Derivatives of Activation Functions\n",
    "---\n",
    "- The Gradient is a derivation of the acttivation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
